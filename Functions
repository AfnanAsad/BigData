import java.util.regex.Pattern
def decimalStrip(inputValue: String, decimal_point: String): String = {
    if (inputValue == null) return null
    val filter = inputValue.replaceAll("[^0-9-\\" + decimal_point + "]", "")

    if (!(filter == "")) {
      val regx = "^(-)(0+)(.*)|^(0+)(.*)"
      val `match` = Pattern.compile(regx).matcher(filter)
      if (`match`.find) if (`match`.group(1) != null) return `match`.group(1) + `match`.group(3)
      else return `match`.group(5)
      return filter
    }
    "0"
  }


 def stringFilter(inputValue: String, searchString: String): String = {
    if (inputValue == null) return null
    val inputChars = inputValue.toCharArray
    val searchStringChars = searchString.toCharArray
    val returnset = new util.LinkedList[Character]
    var i = 0
    while (i < inputChars.length) {
      {
        var j = 0
        while (j < searchStringChars.length) {
          {
            if (inputChars(i) == searchStringChars(j)) {
              returnset.add(inputChars(i))
              break //todo: break is not supported
            }
          }
          {
            j += 1; j - 1
          }
        }
      }
      {
        i += 1; i - 1
      }
    }
    var returnString = ""
    import scala.collection.JavaConversions._
    for (c <- returnset) {
      returnString += c
    }
    returnString
  }


def stringLeftTrim(inputValue: String): String = {
    if (inputValue == null) return null
    inputValue.replaceFirst("\\s+", "")
  }

https://alvinalexander.com/scala/how-find-difference-intersection-distinct-characters-in-string

  val rdd:RDD[CassandraRow]=spark.sparkContext.parallelize(Seq(
      CassandraRow.fromMap(Map("puid"->"p1","vuid"->"v1","e1"->1,"e2"->null,"e3"->1,"edate"->"2015-01-02 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p1","vuid"->"v2","e1"->null,"e2"->1,"e3"->1,"edate"->"2016-01-02 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p1","vuid"->"v3","e1"->null,"e2"->null,"e3"->1,"edate"->"2017-01-02 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p2","vuid"->"v1","e1"->1,"e2"->null,"e3"->null,"edate"->"2017-08-02 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p2","vuid"->"v2","e1"->1,"e2"->1,"e3"->null,"edate"->"2017-01-06 00:00:00.000")),
      CassandraRow.fromMap(Map("puid"->"p3","vuid"->"v1","e1"->1,"e2"->null,"e3"->null,"edate"->"2017-01-06 00:00:00.000"))
    ))

  countElement(rdd,"e1","e2","e3")
  

  def countElement(rdd1:RDD[CassandraRow],elements:String*)
  {
    val countList=rdd1.filter({
      r=> elements.filter(element=> !r.isNullAt(element)).size>1
    }).map(r=>(r.getString("puid"),r.getDate("edate"))).collect()
  rdd1.filter(r=> countList.exists(x=>x._1.equalsIgnoreCase(r.getString("puid")) && (r.getDate("edate").after(x._2) ||r.getDate("edate").equals(x._2)))).foreach(println)
  }




 object Main extends App {

    object WeekDay extends Enumeration {
      type WeekDay = Value
      val Mon, Tue, Wed, Thu, Fri, Sat, Sun = Value
    }
    import WeekDay._

    def isWorkingDay(d: WeekDay) = ! (d == Sat || d == Sun)

    WeekDay.values filter isWorkingDay foreach println
  }


    object WeekDay extends Enumeration {
      type WeekDay = Value
      val Sat=Value("Sat")
      val Fri=Value("ge")
      val Sun=Value("Sun")
      //val Mon, Tue, Wed, Thu, Fri, Sat, Sun = Val
    }
    import WeekDay._
    isWorkingDay(WeekDay.Sun)
    def isWorkingDay(d: WeekDay.Value): Unit ={
      println(d)
    }



    /*val df = spark.createDataFrame(Seq(
      (0, "a1", "b1", "c1", "d1"),
      (1, "a2", "b2", "c2", "d2"),
      (2, "a3", "b3", null, "d3"),
      (3, null, null, "c4", "d4"),
      (4, null, "b5", "c5", "d5"),
      (5, null, "b5", "c5", "d5")
    )).toDF("id", "col1", "col2", "col3", "col4")*/

    val df = spark.createDataFrame(Seq(
      (0, "a1", "b1", "12f-puhrns-3435", "d1"),
      (1, "a2", "b2", "12f-puhrns-343556", "d2"),
      (2, "a3", "b3", null, "d3"),
      (3, null, null, "sdfsd-234-6787", "d4"),
      (4, null, "b5", "12f-puhrns-343522", "d5"),
      (5, null, "b5", "12f-puhrns-343522", "d5")
    )).toDF("id", "col1", "col2", "col3", "col4")

    val fieldValue=Array("12f-puhrns-343522","12f-puhrns-343556")

    df.where(df.col("col3").isin(fieldValue:_*)).show()


 val fieldValue=Array("12f-puhrns-343522","12f-puhrns-343556")

    //df.where("col3 =='12f-puhrns-343522' or col3 =='12f-puhrns-343556'").show()

    //df.where("col3 in ('12f-puhrns-343522','12f-puhrns-343556')").show()
    val cond="col3 in ("+fieldValue.map(x=>s"'$x'").mkString(",")+")"
    println(cond)
    df.where(cond).show()
df.where(cond +" or "+"col4 in ('d3')").show()



data = [(1,340.0,10),(1,None,34),(3,200.0,67),(4,np.NAN,86)]
df = spark.createDataFrame(data).toDF(*["a","b","c"]).withColumn("ar",array("a","b","c")).withColumn("ar2",expr("filter(ar, x -> ((x IS NOT NULL) AND (!isNan(x))))"))
